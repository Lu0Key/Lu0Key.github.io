<!doctype html>
<html lang="en-us">
  <head>
    <title>DDP 多卡训练样例代码 // 洛七的摸鱼池塘</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.81.0" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="洛七" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.0fb49e70a30412f97ddfc418e18fefef1d9fcdebe45f634dbbba768b00fe1eec.css" />
    

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="DDP 多卡训练样例代码"/>
<meta name="twitter:description" content="有时候训练模型为了提高速度，单纯的提高batch size 已经没用了, 这时候我们会使用多卡训练，这里我们使用DDP（Distributed Data Parallel）来实现多卡训练。
这里是单机多卡, 毕竟我们集群上也没几台机器&hellip;
DDP 比较麻烦, 因此这里记录一下如何使用DDP进行多卡训练。
# 这里只是一个大致的样例，具体的代码需要根据实际情况进行修改 # 有些常见的包我也不写了 import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP parser = argparse.ArgumentParser(description=&#34;PyTorch DDP Example&#34;) # 这个是我的习惯, 常常是为了适配单卡和多卡的场景 parser.add_argument(&#34;--is_multi_gpu&#34;, type=bool, default=False) # 下面这个是必要的, 这个参数不需要手动传入 parser.add_argument(&#34;--local-rank&#34;, type=int, default=-1) # 好像2.0 开始的torch 才是 local-rank # 1.x 的是 local_rank args = parser.parse_args() device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)# 指定gpu0为主GPU is_multi_gpu = args.is_multi_gpu # 后面全是用这种方式去进行适配单卡和多卡 if is_multi_gpu == True: device = args."/>

    <meta property="og:title" content="DDP 多卡训练样例代码" />
<meta property="og:description" content="有时候训练模型为了提高速度，单纯的提高batch size 已经没用了, 这时候我们会使用多卡训练，这里我们使用DDP（Distributed Data Parallel）来实现多卡训练。
这里是单机多卡, 毕竟我们集群上也没几台机器&hellip;
DDP 比较麻烦, 因此这里记录一下如何使用DDP进行多卡训练。
# 这里只是一个大致的样例，具体的代码需要根据实际情况进行修改 # 有些常见的包我也不写了 import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP parser = argparse.ArgumentParser(description=&#34;PyTorch DDP Example&#34;) # 这个是我的习惯, 常常是为了适配单卡和多卡的场景 parser.add_argument(&#34;--is_multi_gpu&#34;, type=bool, default=False) # 下面这个是必要的, 这个参数不需要手动传入 parser.add_argument(&#34;--local-rank&#34;, type=int, default=-1) # 好像2.0 开始的torch 才是 local-rank # 1.x 的是 local_rank args = parser.parse_args() device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)# 指定gpu0为主GPU is_multi_gpu = args.is_multi_gpu # 后面全是用这种方式去进行适配单卡和多卡 if is_multi_gpu == True: device = args." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Lu0key.github.io/post/example-code-for-multi-gpu-training-using-ddp/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-04-01T10:32:46&#43;08:00" />
<meta property="article:modified_time" content="2024-04-01T10:32:46&#43;08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://Lu0key.github.io/"><img class="app-header-avatar" src="/avatar.jpg" alt="洛七" /></a>
      <span class="app-header-title">洛七的摸鱼池塘</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/tags/">Tags</a>
      </nav>
      <p>即将没书读的咸鱼</p>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">DDP 多卡训练样例代码</h1>
      <div class="post-meta">
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Apr 1, 2024
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          2 min read
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://Lu0key.github.io/tags/python/">Python</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>有时候训练模型为了提高速度，单纯的提高batch size 已经没用了, 这时候我们会使用多卡训练，这里我们使用DDP（Distributed Data Parallel）来实现多卡训练。</p>
<p>这里是单机多卡, 毕竟我们集群上也没几台机器&hellip;</p>
<p>DDP 比较麻烦, 因此这里记录一下如何使用DDP进行多卡训练。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 这里只是一个大致的样例，具体的代码需要根据实际情况进行修改</span>
<span style="color:#75715e"># 有些常见的包我也不写了</span>

<span style="color:#f92672">import</span> torch.distributed <span style="color:#f92672">as</span> dist
<span style="color:#f92672">from</span> torch.nn.parallel <span style="color:#f92672">import</span> DistributedDataParallel <span style="color:#66d9ef">as</span> DDP

parser <span style="color:#f92672">=</span> argparse<span style="color:#f92672">.</span>ArgumentParser(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;PyTorch DDP Example&#34;</span>)
<span style="color:#75715e"># 这个是我的习惯, 常常是为了适配单卡和多卡的场景</span>
parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--is_multi_gpu&#34;</span>, type<span style="color:#f92672">=</span>bool, default<span style="color:#f92672">=</span>False)
<span style="color:#75715e"># 下面这个是必要的, 这个参数不需要手动传入</span>
parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--local-rank&#34;</span>, type<span style="color:#f92672">=</span>int, default<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
<span style="color:#75715e"># 好像2.0 开始的torch 才是 local-rank</span>
<span style="color:#75715e"># 1.x 的是 local_rank</span>
args <span style="color:#f92672">=</span> parser<span style="color:#f92672">.</span>parse_args()

device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)<span style="color:#75715e"># 指定gpu0为主GPU</span>
is_multi_gpu <span style="color:#f92672">=</span> args<span style="color:#f92672">.</span>is_multi_gpu
<span style="color:#75715e"># 后面全是用这种方式去进行适配单卡和多卡</span>
<span style="color:#66d9ef">if</span> is_multi_gpu <span style="color:#f92672">==</span> True:
    device <span style="color:#f92672">=</span> args<span style="color:#f92672">.</span>local_rank
    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>set_device(args<span style="color:#f92672">.</span>local_rank)
    dist<span style="color:#f92672">.</span>init_process_group(backend<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;nccl&#39;</span>)

<span style="color:#75715e"># 首先加载数据集</span>
train_dataset <span style="color:#f92672">=</span> <span style="color:#f92672">...</span>
train_dataloader <span style="color:#f92672">=</span> DataLoader(train_dataset, batch_size<span style="color:#f92672">=...</span>, shuffle<span style="color:#f92672">=</span>True, num_workers<span style="color:#f92672">=...</span>)
<span style="color:#66d9ef">if</span> is_multi_gpu <span style="color:#f92672">==</span> True:
    train_sampler <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>distributed<span style="color:#f92672">.</span>DistributedSampler(train_dataset)
    train_dataloader <span style="color:#f92672">=</span> DataLoader(train_dataset, batch_size<span style="color:#f92672">=...</span>, sampler<span style="color:#f92672">=</span>train_sampler, num_workers<span style="color:#f92672">=...</span>)


<span style="color:#75715e"># 超級重要!</span>
<span style="color:#75715e"># 如果你要load 模型, 那么你需要在这里加上这个</span>
<span style="color:#75715e"># model.load_state_dict(torch.load(&#34;model.pth&#34;, map_location=&#34;cpu&#34;))</span>
<span style="color:#75715e"># 不然你的卡一會多占用很多份的顯存,份數為 GPU 數量 - 1</span>
model <span style="color:#f92672">=</span> <span style="color:#f92672">...</span>
model <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>to(device)
<span style="color:#66d9ef">if</span> is_multi_gpu <span style="color:#f92672">==</span> True:
    model <span style="color:#f92672">=</span> DDP(model, device_ids<span style="color:#f92672">=</span>[args<span style="color:#f92672">.</span>local_rank], output_device<span style="color:#f92672">=</span>args<span style="color:#f92672">.</span>local_rank)
    <span style="color:#75715e"># 一般会加上另一个参数(因为有些参数不会回归)</span>
    model <span style="color:#f92672">=</span> DDP(model, device_ids<span style="color:#f92672">=</span>[args<span style="color:#f92672">.</span>local_rank], output_device<span style="color:#f92672">=</span>args<span style="color:#f92672">.</span>local_rank, find_unused_parameters<span style="color:#f92672">=</span>True)



optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">5e-5</span>)

<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(epochs):
    <span style="color:#66d9ef">if</span> is_multi_gpu:
        train_dataloader<span style="color:#f92672">.</span>sampler<span style="color:#f92672">.</span>set_epoch(epoch)
    train_bar <span style="color:#f92672">=</span> tqdm(train_dataloader, desc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;训练&#34;</span>)
    model<span style="color:#f92672">.</span>train()
    <span style="color:#66d9ef">for</span> step, batch <span style="color:#f92672">in</span> enumerate(train_bar):
        <span style="color:#f92672">...</span>
        <span style="color:#f92672">...</span>
        <span style="color:#f92672">...</span>
        train_bar<span style="color:#f92672">.</span>update(<span style="color:#ae81ff">1</span>)

<span style="color:#75715e"># 结束后要销毁</span>
<span style="color:#66d9ef">if</span> is_multi_gpu <span style="color:#f92672">==</span> True:
    dist<span style="color:#f92672">.</span>destroy_process_group()

<span style="color:#75715e"># 如果要保存模型应该保存的是 model.module 而不是 model</span>
<span style="color:#75715e"># torch.save(model.module.state_dict(), &#34;model.pth&#34;)</span>
</code></pre></div><p>大致是上面这样一个流程，当然具体的代码需要根据实际情况进行修改. 之前遇到一个 bug, 就是在集群上使用多卡进行训练的时候, 卡越多, 训练的速度越慢, 这是因为在集群上训练的时候需要设置一个参数, <code>#SBATCH --ntasks-per-node=8</code>, 这里的8是因为我使用了8张卡, 如果你使用的是4张卡, 那么这里就是4.</p>
<p>原因是你使用的几张卡, 他就会启动几个进程, 但是之前默认的线程数是1, 所以好像会有问题, 可能会阻塞? maybe, 我也不是很清楚, 但是设置了这个参数之后, 训练速度就恢复正常了, 每增加一张卡都能提高一些训练速度.</p>
<p>然后是启动训练的脚本</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#75715e"># 我这里实验用环境的是</span>
<span style="color:#75715e"># torch 2.0.1</span>
<span style="color:#75715e"># 这里的 8 是因为我使用了8张卡</span>
<span style="color:#75715e"># --local-rank 不需要手动传入</span>
python -m torch.distributed.launch --nproc_per_node <span style="color:#ae81ff">8</span> xxx.py --is_multi_gpu True
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 由于启动后, 会启动和显卡数一样多的线程, 因此会导致你线程里面一旦输出了什么, 就会输出 8份的内容,保存文件也是一样, 会保存8份(虽然会覆盖)‘</span>
<span style="color:#75715e"># 因此需要在你的代码里面加上这个</span>
<span style="color:#66d9ef">if</span> (is_multi_gpu <span style="color:#f92672">==</span> True <span style="color:#f92672">and</span> dist<span style="color:#f92672">.</span>get_rank() <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">or</span> is_multi_gpu <span style="color:#f92672">==</span> False:
    <span style="color:#66d9ef">pass</span>
<span style="color:#75715e"># 这样就只会输出一份了</span>
<span style="color:#75715e"># 当我要保存什么, 或者输出什么不想输出多份的时候就用这个</span>
</code></pre></div><p>题外话, 我这个代码需要用多线程进行处理, 所以可以在代码里面加</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">CPU_THREAD_NUM <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>
os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;OMP_NUM_THREADS&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;{}&#34;</span><span style="color:#f92672">.</span>format(CPU_THREAD_NUM)
torch<span style="color:#f92672">.</span>set_num_threads(CPU_THREAD_NUM)
<span style="color:#75715e"># 然后你启动后的每个线程可以用 8 个CPU线程</span>
<span style="color:#75715e"># 至少我的会快一点</span>
</code></pre></div><p>然后抢节点前需要看一下节点的CPU资源, 希望别因为CPU需要的太多, 导致一直等, 毕竟卡更重要一些.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#75715e"># gpu002 可以换成你想看的节点</span>
scontrol show node gpu002 -o | grep -oP <span style="color:#e6db74">&#39;CPUAlloc=\K[^ ]+|CPUTot=\K[^ ]+&#39;</span> | awk <span style="color:#e6db74">&#39;BEGIN { OFS=&#34;=&#34; } { if (NR % 2 == 0) { print &#34;已分配&#34;, $0 } else { print &#34;总共&#34;, $0 } }&#39;</span>
</code></pre></div><p>你可能会需要?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#75715e"># register</span>
<span style="color:#75715e"># model.register_buffer(&#39;step&#39;, torch.tensor(0))</span>

<span style="color:#75715e"># DDP 会封装原模型的 forward</span>
<span style="color:#75715e"># 如果你的模型里面有其他的操作, 比如生成操作, 那么你需要手动调用</span>
<span style="color:#75715e"># model.module.原模型的函数()</span>
</code></pre></div>
    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
