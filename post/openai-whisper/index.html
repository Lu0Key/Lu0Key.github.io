<!doctype html>
<html lang="en-us">
  <head>
    <title>Openai的Whisper语音转文字试用 // 洛七的摸鱼池塘</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.81.0" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="洛七" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.0fb49e70a30412f97ddfc418e18fefef1d9fcdebe45f634dbbba768b00fe1eec.css" />
    

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Openai的Whisper语音转文字试用"/>
<meta name="twitter:description" content="openai whisper 试用 conda create -n ow python=3.9 conda activate ow pip install git&#43;https://github.com/openai/whisper.git@ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab certifi==2024.2.2 charset-normalizer==3.3.2 filelock==3.13.1 fsspec==2024.2.0 idna==3.6s Jinja2==3.1.3 Levenshtein==0.25.0 llvmlite==0.42.0 MarkupSafe==2.1.5 more-itertools==10.2.0 mpmath==1.3.0 networkx==3.2.1 numba==0.59.0 numpy==1.26.4 nvidia-cublas-cu12==12.1.3.1 nvidia-cuda-cupti-cu12==12.1.105 nvidia-cuda-nvrtc-cu12==12.1.105 nvidia-cuda-runtime-cu12==12.1.105 nvidia-cudnn-cu12==8.9.2.26 nvidia-cufft-cu12==11.0.2.54 nvidia-curand-cu12==10.3.2.106 nvidia-cusolver-cu12==11.4.5.107 nvidia-cusparse-cu12==12.1.0.106 nvidia-nccl-cu12==2.19.3 nvidia-nvjitlink-cu12==12.3.101 nvidia-nvtx-cu12==12.1.105 openai-whisper @ git&#43;https://github.com/openai/whisper.git@ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab python-Levenshtein==0.25.0 rapidfuzz==3.6.1 regex==2023.12.25 requests==2.31.0 sympy==1.12 tiktoken==0.6.0 torch==2.2.1 tqdm==4.66.2 triton==2.2.0 typing_extensions==4.10.0 urllib3==2.2.1 以下是测试脚本.
import whisper import time import os audio_name = &#34;1.wav&#34; model_type = &#34;base&#34; model = whisper.load_model(model_type) print(model."/>

    <meta property="og:title" content="Openai的Whisper语音转文字试用" />
<meta property="og:description" content="openai whisper 试用 conda create -n ow python=3.9 conda activate ow pip install git&#43;https://github.com/openai/whisper.git@ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab certifi==2024.2.2 charset-normalizer==3.3.2 filelock==3.13.1 fsspec==2024.2.0 idna==3.6s Jinja2==3.1.3 Levenshtein==0.25.0 llvmlite==0.42.0 MarkupSafe==2.1.5 more-itertools==10.2.0 mpmath==1.3.0 networkx==3.2.1 numba==0.59.0 numpy==1.26.4 nvidia-cublas-cu12==12.1.3.1 nvidia-cuda-cupti-cu12==12.1.105 nvidia-cuda-nvrtc-cu12==12.1.105 nvidia-cuda-runtime-cu12==12.1.105 nvidia-cudnn-cu12==8.9.2.26 nvidia-cufft-cu12==11.0.2.54 nvidia-curand-cu12==10.3.2.106 nvidia-cusolver-cu12==11.4.5.107 nvidia-cusparse-cu12==12.1.0.106 nvidia-nccl-cu12==2.19.3 nvidia-nvjitlink-cu12==12.3.101 nvidia-nvtx-cu12==12.1.105 openai-whisper @ git&#43;https://github.com/openai/whisper.git@ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab python-Levenshtein==0.25.0 rapidfuzz==3.6.1 regex==2023.12.25 requests==2.31.0 sympy==1.12 tiktoken==0.6.0 torch==2.2.1 tqdm==4.66.2 triton==2.2.0 typing_extensions==4.10.0 urllib3==2.2.1 以下是测试脚本.
import whisper import time import os audio_name = &#34;1.wav&#34; model_type = &#34;base&#34; model = whisper.load_model(model_type) print(model." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Lu0key.github.io/post/openai-whisper/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-03-10T20:26:19&#43;08:00" />
<meta property="article:modified_time" content="2024-03-10T20:26:19&#43;08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://Lu0key.github.io/"><img class="app-header-avatar" src="/avatar.jpg" alt="洛七" /></a>
      <span class="app-header-title">洛七的摸鱼池塘</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/tags/">Tags</a>
      </nav>
      <p>即将没书读的咸鱼</p>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Openai的Whisper语音转文字试用</h1>
      <div class="post-meta">
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Mar 10, 2024
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://Lu0key.github.io/tags/python/">Python</a>
              <a class="tag" href="https://Lu0key.github.io/tags/llm/">LLM</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <h2 id="openai-whisper-试用">openai whisper 试用</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">conda create -n ow python<span style="color:#f92672">=</span>3.9
conda activate ow
pip install git+https://github.com/openai/whisper.git@ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab
</code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">certifi==2024.2.2
charset-normalizer==3.3.2
filelock==3.13.1
fsspec==2024.2.0
idna==3.6s
Jinja2==3.1.3
Levenshtein==0.25.0
llvmlite==0.42.0
MarkupSafe==2.1.5
more-itertools==10.2.0
mpmath==1.3.0
networkx==3.2.1
numba==0.59.0
numpy==1.26.4
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.19.3
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu12==12.1.105
openai-whisper @ git+https://github.com/openai/whisper.git@ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab
python-Levenshtein==0.25.0
rapidfuzz==3.6.1
regex==2023.12.25
requests==2.31.0
sympy==1.12
tiktoken==0.6.0
torch==2.2.1
tqdm==4.66.2
triton==2.2.0
typing_extensions==4.10.0
urllib3==2.2.1
</code></pre></div><!-- raw HTML omitted -->
<p>以下是测试脚本.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> whisper
<span style="color:#f92672">import</span> time
<span style="color:#f92672">import</span> os
audio_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;1.wav&#34;</span>

model_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;base&#34;</span>

model <span style="color:#f92672">=</span> whisper<span style="color:#f92672">.</span>load_model(model_type)

<span style="color:#66d9ef">print</span>(model<span style="color:#f92672">.</span>device)
audio <span style="color:#f92672">=</span> whisper<span style="color:#f92672">.</span>load_audio(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(audio_name))
audio <span style="color:#f92672">=</span> whisper<span style="color:#f92672">.</span>pad_or_trim(audio)

start_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()

<span style="color:#75715e"># make log-Mel spectrogram and move to the same device as the model</span>
<span style="color:#75715e"># 如果要跑 large 记得要用这个代码设置成 128</span>
<span style="color:#75715e"># mel = whisper.log_mel_spectrogram(audio, n_mels=128).to(model.device)</span>
mel <span style="color:#f92672">=</span> whisper<span style="color:#f92672">.</span>log_mel_spectrogram(audio)<span style="color:#f92672">.</span>to(model<span style="color:#f92672">.</span>device)

<span style="color:#75715e"># 检测语言种类</span>
_, probs <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>detect_language(mel)
<span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#34;Detected language: {max(probs, key=probs.get)}&#34;</span>)

<span style="color:#75715e"># decode the audio</span>
options <span style="color:#f92672">=</span> whisper<span style="color:#f92672">.</span>DecodingOptions()
result <span style="color:#f92672">=</span> whisper<span style="color:#f92672">.</span>decode(model, mel, options)

<span style="color:#75715e"># print the recognized text</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;识别结果: &#34;</span>, result<span style="color:#f92672">.</span>text)
</code></pre></div><p>一次性就测试成功了.</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
