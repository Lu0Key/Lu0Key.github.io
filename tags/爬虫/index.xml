<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>爬虫 on 洛七的摸鱼池塘</title>
    <link>https://Lu0key.github.io/tags/%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 爬虫 on 洛七的摸鱼池塘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Jan 2023 01:47:56 +0800</lastBuildDate><atom:link href="https://Lu0key.github.io/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>科学上网爬虫</title>
      <link>https://Lu0key.github.io/post/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E7%88%AC%E8%99%AB/</link>
      <pubDate>Mon, 09 Jan 2023 01:47:56 +0800</pubDate>
      
      <guid>https://Lu0key.github.io/post/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E7%88%AC%E8%99%AB/</guid>
      <description>有时候会有需求需要爬需要科学上网才能访问的网站，但是如果开全局模式，访问正常网站就会非常慢，因此需要开PAC模式，但是PAC模式似乎不能直接用 requests 发送请求。
找了半天，最后发现了PAC模式支持的方式，代码如下
from pypac import PACSession,get_pac url = &amp;#39;https://xxxxxx.com&amp;#39; # 这个 url 是要看你的科学上网软件来着的 pac = get_pac(url=&amp;#39;http://127.0.0.1:端口/pac/?t=164907&amp;#39;) s = PACSession(pac) #解析pac文件 resp = s.get(url, headers=headers) </description>
    </item>
    
    <item>
      <title>纪念健康打卡</title>
      <link>https://Lu0key.github.io/post/%E7%BA%AA%E5%BF%B5%E5%81%A5%E5%BA%B7%E6%89%93%E5%8D%A1/</link>
      <pubDate>Sat, 07 Jan 2023 23:24:14 +0800</pubDate>
      
      <guid>https://Lu0key.github.io/post/%E7%BA%AA%E5%BF%B5%E5%81%A5%E5%BA%B7%E6%89%93%E5%8D%A1/</guid>
      <description>疫情管控放开了，学校也终于取消健康打卡了，之前写的自动化健康打卡也用不到了，记录一下这个健康打卡引用中遇到的两个新问题
这里的代码是用selenium实现的，因为懒，不想再分析请求了
第一个问题：如何发送邮件
from email.mime.text import MIMEText #纯文本，HTML from email.mime.image import MIMEImage #图片 from email.mime.multipart import MIMEMultipart #多种组合内容 # smtplib 用于邮件的发信动作 import smtplib # email 用于构建邮件内容 from email.mime.text import MIMEText # 构建邮件头 from email.header import Header def sendMail(status): # 发信方的信息：发信邮箱，QQ 邮箱授权码 # 发送方邮箱 from_addr = &amp;#39;xxxxx@qq.com&amp;#39; # 发送方的QQ邮箱授权码 password = &amp;#39;asdfghjkl&amp;#39; # 收信方邮箱 to_addr = &amp;#39;yyyyyy@qq.com&amp;#39; # 发信服务器 # QQ那边设置 smtp_server = &amp;#39;smtp.qq.com&amp;#39; # 邮箱正文内容，第一个参数为内容，第二个参数为格式(plain 为纯文本)，第三个参数为编码 msg = MIMEText(&amp;#39;恭喜你今天的打卡完成啦！&amp;#39;, &amp;#39;plain&amp;#39;, &amp;#39;utf-8&amp;#39;) # 邮件头信息 msg[&amp;#39;From&amp;#39;] = Header(&amp;#39;打卡小助手&amp;#39;) # 发送者 msg[&amp;#39;To&amp;#39;] = Header(&amp;#39;洛七&amp;#39;) # 接收者 subject = &amp;#39;【健康打卡】&amp;#39;+status msg[&amp;#39;Subject&amp;#39;] = Header(subject, &amp;#39;utf-8&amp;#39;) # 邮件主题 try: smtpobj = smtplib.</description>
    </item>
    
    <item>
      <title>Python 下载图片</title>
      <link>https://Lu0key.github.io/post/python-download-img/</link>
      <pubDate>Sun, 11 Jul 2021 00:23:25 +0800</pubDate>
      
      <guid>https://Lu0key.github.io/post/python-download-img/</guid>
      <description>在做Python 爬虫的时候，爬取图片是一个很常用的功能
# 主要部分代码 with open(filename, &amp;#34;wb&amp;#34;) as f: f.write(resp.content) 比较通用的下载图片代码
def download_img(url,src,count=0): &amp;#34;&amp;#34;&amp;#34; :param url: 图片地址 :param src: 图片存放的文件夹 :param count: 调用次数，默认为 0 &amp;#34;&amp;#34;&amp;#34; resp = requests.get(url,headers=headers) filename = src + &amp;#34;\\&amp;#34;+ url.split(&amp;#34;/&amp;#34;)[-1] # 判断文件夹存不存在 if not os.path.exists(src): os.makedirs(src) if not os.path.isdir(src): os.makedirs(src) try: # 主要部分 with open(filename, &amp;#34;wb&amp;#34;) as f: f.write(resp.content) resp.close() except: print(&amp;#34;============================================&amp;#34;) print(&amp;#34;Error:&amp;#34;, filename) resp.close() if count&amp;lt;5: download_img(url,src,count+1) else: print(&amp;#34;彻底失败&amp;#34;) </description>
    </item>
    
    <item>
      <title>Python 爬虫乱码问题</title>
      <link>https://Lu0key.github.io/post/python-crawler-mistaken-code/</link>
      <pubDate>Sat, 10 Jul 2021 23:23:46 +0800</pubDate>
      
      <guid>https://Lu0key.github.io/post/python-crawler-mistaken-code/</guid>
      <description>import requests headers = { &amp;#34;User-Agent&amp;#34;:&amp;#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0&amp;#34; } resp = requests.get(&amp;#34;http://cmathc.cn/&amp;#34;, headers=headers) print(resp.text) 可以很明显看出有乱码，解决的方式非常的简单粗暴
我们发现，网络返回的字符集类型和推测出来的字符集类型是不一致的，因此我们只要将字符集类型设定为推断出来的字符集类型即可，于是
import requests headers = { &amp;#34;User-Agent&amp;#34;:&amp;#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0&amp;#34; } resp = requests.get(&amp;#34;http://cmathc.cn/&amp;#34;, headers=headers) resp.encoding = resp.apparent_encoding print(resp.text) 看图可知乱码问题得到了解决</description>
    </item>
    
  </channel>
</rss>
